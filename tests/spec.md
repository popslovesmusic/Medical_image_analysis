# Module Specification

**Path:** tests
**Generated by:** Codex cleanup task
**Date:** 2025-10-31


Specification: Unified Testing Framework

Module Path: tests/
Parent Specs:

core/spec.md (defines core subsystems and interfaces)

experiments/spec.md (links experimental results to test verification)

trainer/spec.md (trainer logic and model validation)

core/diagnostics/spec.md (metrics and validation methods)

I. Mission

The Unified Testing Framework guarantees functional correctness, numerical determinism, and cross-module stability throughout the project.

Its purpose is to ensure:

Every algorithm behaves identically across runs and environments.

Integration between modules (tensor, bridge, diagnostics, dream, etc.) remains stable.

Regression detection is automated via archived baselines.

Testing artifacts are fully traceable and reproducible.

II. Directory Layout
tests/
├─ unit/                 # Individual module unit tests
├─ integration/          # Multi-module end-to-end tests
├─ regression/           # Baseline consistency checks
├─ performance/          # Timing and resource validation
├─ fixtures/             # Synthetic input and mock data
├─ logs/                 # Test run logs, errors, and timing reports
├─ reports/              # Markdown/JSON summaries of test results
└─ utils/                # Shared helpers and deterministic test seeds

III. Test Hierarchy Overview
Level	Purpose	Scope
Unit Tests	Validate internal module correctness	core/src/…, trainer/src/…
Integration Tests	Verify modules interoperate correctly	bridge, diagnostics, dream
Regression Tests	Detect output drift over time	archived result deltas
Performance Tests	Ensure consistent runtime/resource use	timing & memory profile
Fixtures & Mocks	Provide stable, known input data	synthetic tensors & mini datasets
IV. Deterministic Execution Rules
Constraint	Enforcement
Global Seed	42
Parallelism	Disabled for test runs
Randomization	Re-initialized per test, fixed seed
Order of Tests	Alphabetical; enforced via pytest --disable-warnings --maxfail=1
Precision Tolerance	Float Δ ≤ 1e⁻⁶ (unit), ≤ 1e⁻⁸ (integration)
Hash Verification	SHA-256 comparison for serialized outputs
Time Zone	UTC only for all timestamps
V. File Naming Convention

Format:

test_<module>_<purpose>.py


Examples:

test_tensor_ops.py

test_dream_pool_retrieval.py

test_bridge_roundtrip.py

test_diagnostics_metrics.py

test_trainer_validation.py

VI. Test Categories
1. Unit Tests (tests/unit/)

Each functional block in core/src or trainer/src has a mirrored test file verifying:

Function output correctness

Error and edge cases

Numeric determinism

Expected exceptions or boundary handling

Example:

def test_tensor_normalization():
    from core.tensor import ChromaticTensor
    t = ChromaticTensor.new_mock(0.5, 0.3, 0.7)
    norm = t.normalize()
    assert abs(norm.mean_rgb()[0] - 0.5) < 1e-6

2. Integration Tests (tests/integration/)

Validates dataflow across modules:

Color → Frequency conversion (Bridge)

Diagnostics feedback through Continuity Planner

DreamPool + Learner loop end-to-end

Example:

def test_chromatic_to_sonic_roundtrip():
    from core.bridge import hue_to_frequency, frequency_to_hue
    import numpy as np
    h = np.linspace(0, 2*np.pi, 12)
    f = hue_to_frequency(h)
    h2 = frequency_to_hue(f)
    assert np.allclose(h, h2, atol=1e-6)

3. Regression Tests (tests/regression/)

Ensures outputs match archived baseline metrics.

Step	Description
Load baseline JSON	from experiments/results/metrics/
Execute corresponding test function	with identical inputs
Compute Δ	compare to baseline
Assert drift below threshold	Δ ≤ 1e⁻⁶
Record in regression report	with pass/fail summary
4. Performance Tests (tests/performance/)

Verifies stable execution time and memory footprint using deterministic profiling.

Example threshold:

Metric	Target	Tolerance
Runtime	< 500 ms	±5%
Memory	< 512 MB	±2%
GPU Util	< 60%	±3%

Performance variance > tolerance flags a “⚠️ performance drift”.

5. Fixtures (tests/fixtures/)

Provides consistent mock data for reproducible testing.

Fixture	Description
mock_tensor.npy	Random RGB arrays with fixed seed
mock_metrics.json	Precomputed baseline diagnostics
mock_config.yaml	Minimal experiment configuration
mock_chromatic_sonic_pair.pkl	Deterministic color–frequency mappings

All fixtures are generated via scripts/generate_fixtures.py.

VII. Test Execution Pipeline
Phase	Tool	Output
1. Discovery	pytest --collect-only	Test inventory
2. Execution	pytest --maxfail=1 --disable-warnings	Pass/fail
3. Coverage	pytest --cov=core	Coverage report
4. Report	pytest-json	reports/tests_summary.json
5. Export	Markdown + HTML	reports/tests_summary.md
VIII. Continuous Integration (CI)

The test suite is run automatically via GitHub Actions or local CI, defined in .github/workflows/test.yml.

Stages:

Environment setup (from environment.yml)

Linting with flake8 and black

Unit and integration tests

Regression test comparison

Coverage upload to /docs/reports/coverage.md

Failing any stage blocks merging to main.

IX. Output Artifacts
File	Description
tests/reports/tests_summary.json	Machine-readable results
tests/reports/tests_summary.md	Human-readable summary
tests/logs/*.log	Raw logs for each test file
tests/fixtures/*.npy	Generated input data
tests/baselines/*.json	Archived baselines
tests/coverage/coverage.xml	Coverage report for CI

Example report snippet:

# Test Summary — Build 2025-10-31
✅ 402 tests passed
⚠️ 2 skipped (platform-specific)
❌ 0 failed
Coverage: 97.8%
Determinism verified: 100%

X. Deterministic Testing Utilities
Utility	Purpose
tests/utils/seed.py	Initialize all RNGs (Python, NumPy, Torch)
tests/utils/assertions.py	Float equality helpers with deterministic rounding
tests/utils/hashcheck.py	Verify output hash matches baseline
tests/utils/timeguard.py	Enforce runtime bounds per test
XI. Failure Handling Policy
Type	Response
Non-deterministic drift	Rerun test; if persists, flag for audit
Regression delta > tolerance	Archive current run as candidate_baseline
Performance drift	Log warning, notify Codex Diagnostics Agent
Unhandled exception	Abort test batch immediately

All failures generate an entry in tests/logs/failures.log and update the chronicle.

XII. Determinism Validation

Each test run ends with:

pytest --determinism-check


This ensures:

RNG states before/after are identical

File hashes unchanged by tests

Time-dependent functions replaced with mocks

Results are logged in:

tests/logs/determinism_audit.log

XIII. Compliance Rules
Rule	Constraint
RNG Seed	42 (fixed global)
Floating Precision	6 decimal places
File I/O Order	Lexicographically sorted
OS Independence	Must pass on Windows + Linux
Test Log Format	JSON UTF-8
Hash Algorithm	SHA-256
Max Drift	1e⁻⁶ (unit) / 1e⁻⁸ (integration)
CI Blocking	Enabled for any failure
XIV. Compliance Summary
Field	Specification
Spec Version	1.0
Determinism Level	Bit-Exact
Framework	Pytest
Seed	42
Coverage Target	≥95%
Audit Authority	Codex Validation Agent
Revision	{{auto-date}}
Status	✅ Verified Deterministic Test System